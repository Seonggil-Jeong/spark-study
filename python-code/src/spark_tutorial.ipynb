{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-21T00:50:22.897167Z",
     "start_time": "2023-08-21T00:49:53.320272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/21 09:49:56 WARN Utils: Your hostname, SeongGils-MacBook-Pro-2.local resolves to a loopback address: 127.0.0.1; using 192.168.219.101 instead (on interface en0)\n",
      "23/08/21 09:49:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/21 09:49:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/08/21 09:49:59 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, transform\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkTutorial\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/20 11:37:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "23/08/20 11:37:44 INFO SharedState: Warehouse path is 'file:/Users/seonggil/Development/bigdata/spark-study/python-code/src/spark-warehouse'.\n",
      "23/08/20 11:37:44 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/08/20 11:37:44 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/08/20 11:37:44 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/08/20 11:37:44 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "23/08/20 11:37:44 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n"
     ]
    }
   ],
   "source": [
    "my_range = spark.range(1000).toDF(\"my_range\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T02:37:45.794071Z",
     "start_time": "2023-08-20T02:37:42.537553Z"
    }
   },
   "id": "18006f6f7866c9d1"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "my_range = my_range.withColumnRenamed(\"my_range\", \"number\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T02:39:08.563452Z",
     "start_time": "2023-08-20T02:39:08.537261Z"
    }
   },
   "id": "a1ef40f01a0af71b"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "tmp = my_range.where(\"number > 0 and number %2 = 0\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T02:40:51.254216Z",
     "start_time": "2023-08-20T02:40:51.225760Z"
    }
   },
   "id": "812d8677ebf633a4"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/20 11:41:06 INFO CodeGenerator: Code generated in 111.525041 ms\n",
      "23/08/20 11:41:06 INFO DAGScheduler: Registering RDD 19 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "23/08/20 11:41:06 INFO DAGScheduler: Got map stage job 4 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "23/08/20 11:41:06 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (count at NativeMethodAccessorImpl.java:0)\n",
      "23/08/20 11:41:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/08/20 11:41:06 INFO DAGScheduler: Missing parents: List()\n",
      "23/08/20 11:41:06 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/08/20 11:41:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 14.9 KiB, free 434.3 MiB)\n",
      "23/08/20 11:41:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 434.3 MiB)\n",
      "23/08/20 11:41:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:50841 (size: 7.1 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:41:06 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\n",
      "23/08/20 11:41:06 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "23/08/20 11:41:06 INFO YarnScheduler: Adding task set 4.0 with 2 tasks resource profile 0\n",
      "23/08/20 11:41:06 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (localhost, executor 1, partition 0, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()\n",
      "23/08/20 11:41:06 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 5) (localhost, executor 2, partition 1, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()\n",
      "23/08/20 11:41:07 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:50853 (size: 7.1 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:41:07 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:50849 (size: 7.1 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:41:07 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 5) in 496 ms on localhost (executor 2) (1/2)\n",
      "23/08/20 11:41:07 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 527 ms on localhost (executor 1) (2/2)\n",
      "23/08/20 11:41:07 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "23/08/20 11:41:07 INFO DAGScheduler: ShuffleMapStage 4 (count at NativeMethodAccessorImpl.java:0) finished in 0.581 s\n",
      "23/08/20 11:41:07 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/08/20 11:41:07 INFO DAGScheduler: running: Set()\n",
      "23/08/20 11:41:07 INFO DAGScheduler: waiting: Set()\n",
      "23/08/20 11:41:07 INFO DAGScheduler: failed: Set()\n",
      "23/08/20 11:41:07 INFO CodeGenerator: Code generated in 61.33 ms\n",
      "23/08/20 11:41:07 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "23/08/20 11:41:07 INFO DAGScheduler: Got job 5 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/08/20 11:41:07 INFO DAGScheduler: Final stage: ResultStage 6 (count at NativeMethodAccessorImpl.java:0)\n",
      "23/08/20 11:41:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
      "23/08/20 11:41:07 INFO DAGScheduler: Missing parents: List()\n",
      "23/08/20 11:41:07 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[22] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/08/20 11:41:07 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 11.1 KiB, free 434.3 MiB)\n",
      "23/08/20 11:41:07 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.3 MiB)\n",
      "23/08/20 11:41:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:50841 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:41:07 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\n",
      "23/08/20 11:41:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[22] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/08/20 11:41:07 INFO YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "23/08/20 11:41:07 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (localhost, executor 2, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\n",
      "23/08/20 11:41:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:50853 (size: 5.5 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:41:07 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 127.0.0.1:50851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/20 11:41:11 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 3751 ms on localhost (executor 2) (1/1)\n",
      "23/08/20 11:41:11 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "23/08/20 11:41:11 INFO DAGScheduler: ResultStage 6 (count at NativeMethodAccessorImpl.java:0) finished in 3.772 s\n",
      "23/08/20 11:41:11 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/08/20 11:41:11 INFO YarnScheduler: Killing all running tasks in stage 6: Stage finished\n",
      "23/08/20 11:41:11 INFO DAGScheduler: Job 5 finished: count at NativeMethodAccessorImpl.java:0, took 3.797542 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "499"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T02:41:11.530352Z",
     "start_time": "2023-08-20T02:41:06.562325Z"
    }
   },
   "id": "e5982c2f080a70ca"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/20 11:42:00 INFO InMemoryFileIndex: It took 49 ms to list leaf files for 1 paths.\n",
      "23/08/20 11:42:00 INFO InMemoryFileIndex: It took 9 ms to list leaf files for 1 paths.\n",
      "23/08/20 11:42:00 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/08/20 11:42:00 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#34, None)) > 0)\n",
      "23/08/20 11:42:00 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "23/08/20 11:42:00 INFO CodeGenerator: Code generated in 59.641084 ms\n",
      "23/08/20 11:42:00 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.1 KiB, free 434.1 MiB)\n",
      "23/08/20 11:42:00 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 127.0.0.1:50841 in memory (size: 5.3 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:42:00 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 434.1 MiB)\n",
      "23/08/20 11:42:00 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 127.0.0.1:50841 (size: 34.9 KiB, free: 434.3 MiB)\n",
      "23/08/20 11:42:00 INFO SparkContext: Created broadcast 6 from csv at NativeMethodAccessorImpl.java:0\n",
      "23/08/20 11:42:00 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:50849 in memory (size: 5.3 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:42:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/08/20 11:42:00 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 127.0.0.1:50841 in memory (size: 5.0 KiB, free: 434.3 MiB)\n",
      "23/08/20 11:42:00 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:50853 in memory (size: 5.0 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:42:00 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 127.0.0.1:50841 in memory (size: 7.1 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:42:00 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:50849 in memory (size: 7.1 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:42:00 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:50853 in memory (size: 7.1 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:42:00 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:50841 in memory (size: 5.0 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:42:00 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:50849 in memory (size: 5.0 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:42:00 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "23/08/20 11:42:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:50841 in memory (size: 5.0 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:42:00 INFO DAGScheduler: Got job 6 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/08/20 11:42:00 INFO DAGScheduler: Final stage: ResultStage 7 (csv at NativeMethodAccessorImpl.java:0)\n",
      "23/08/20 11:42:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:50853 in memory (size: 5.0 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:42:00 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/08/20 11:42:00 INFO DAGScheduler: Missing parents: List()\n",
      "23/08/20 11:42:00 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[26] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/08/20 11:42:00 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 127.0.0.1:50841 in memory (size: 5.5 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:42:00 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:50853 in memory (size: 5.5 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:42:00 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 11.8 KiB, free 434.2 MiB)\n",
      "23/08/20 11:42:00 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.2 MiB)\n",
      "23/08/20 11:42:00 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:50841 (size: 5.9 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:42:00 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\n",
      "23/08/20 11:42:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[26] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/08/20 11:42:00 INFO YarnScheduler: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "23/08/20 11:42:00 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (localhost, executor 1, partition 0, PROCESS_LOCAL, 4925 bytes) taskResourceAssignments Map()\n",
      "23/08/20 11:42:01 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:50849 (size: 5.9 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:42:01 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:50849 (size: 34.9 KiB, free: 434.4 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/20 11:42:02 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1342 ms on localhost (executor 1) (1/1)\n",
      "23/08/20 11:42:02 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "23/08/20 11:42:02 INFO DAGScheduler: ResultStage 7 (csv at NativeMethodAccessorImpl.java:0) finished in 1.382 s\n",
      "23/08/20 11:42:02 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/08/20 11:42:02 INFO YarnScheduler: Killing all running tasks in stage 7: Stage finished\n",
      "23/08/20 11:42:02 INFO DAGScheduler: Job 6 finished: csv at NativeMethodAccessorImpl.java:0, took 1.436708 s\n",
      "23/08/20 11:42:02 INFO CodeGenerator: Code generated in 35.925541 ms\n",
      "23/08/20 11:42:02 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/08/20 11:42:02 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/08/20 11:42:02 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "23/08/20 11:42:02 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.1 KiB, free 434.0 MiB)\n",
      "23/08/20 11:42:02 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 433.9 MiB)\n",
      "23/08/20 11:42:02 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 127.0.0.1:50841 (size: 34.9 KiB, free: 434.3 MiB)\n",
      "23/08/20 11:42:02 INFO SparkContext: Created broadcast 8 from csv at NativeMethodAccessorImpl.java:0\n",
      "23/08/20 11:42:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/20 11:42:02 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "23/08/20 11:42:02 INFO DAGScheduler: Got job 7 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/08/20 11:42:02 INFO DAGScheduler: Final stage: ResultStage 8 (csv at NativeMethodAccessorImpl.java:0)\n",
      "23/08/20 11:42:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/08/20 11:42:02 INFO DAGScheduler: Missing parents: List()\n",
      "23/08/20 11:42:02 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[32] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/08/20 11:42:02 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 25.4 KiB, free 433.9 MiB)\n",
      "23/08/20 11:42:02 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 11.9 KiB, free 433.9 MiB)\n",
      "23/08/20 11:42:02 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 127.0.0.1:50841 (size: 11.9 KiB, free: 434.3 MiB)\n",
      "23/08/20 11:42:02 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\n",
      "23/08/20 11:42:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[32] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/08/20 11:42:02 INFO YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "23/08/20 11:42:02 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (localhost, executor 2, partition 0, PROCESS_LOCAL, 4925 bytes) taskResourceAssignments Map()\n",
      "23/08/20 11:42:02 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:50853 (size: 11.9 KiB, free: 434.4 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/20 11:42:04 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:50853 (size: 34.9 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:42:05 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 2931 ms on localhost (executor 2) (1/1)\n",
      "23/08/20 11:42:05 INFO YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "23/08/20 11:42:05 INFO DAGScheduler: ResultStage 8 (csv at NativeMethodAccessorImpl.java:0) finished in 2.990 s\n",
      "23/08/20 11:42:05 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/08/20 11:42:05 INFO YarnScheduler: Killing all running tasks in stage 8: Stage finished\n",
      "23/08/20 11:42:05 INFO DAGScheduler: Job 7 finished: csv at NativeMethodAccessorImpl.java:0, took 3.005157 s\n",
      "23/08/20 11:42:05 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/08/20 11:42:05 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/08/20 11:42:05 INFO FileSourceStrategy: Output Data Schema: struct<>\n",
      "23/08/20 11:42:05 INFO CodeGenerator: Code generated in 30.237959 ms\n",
      "23/08/20 11:42:05 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.0 KiB, free 433.7 MiB)\n",
      "23/08/20 11:42:05 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 433.7 MiB)\n",
      "23/08/20 11:42:05 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 127.0.0.1:50841 (size: 34.9 KiB, free: 434.3 MiB)\n",
      "23/08/20 11:42:05 INFO SparkContext: Created broadcast 10 from count at NativeMethodAccessorImpl.java:0\n",
      "23/08/20 11:42:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/08/20 11:42:05 INFO DAGScheduler: Registering RDD 36 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "23/08/20 11:42:05 INFO DAGScheduler: Got map stage job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/08/20 11:42:05 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0)\n",
      "23/08/20 11:42:05 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/08/20 11:42:05 INFO DAGScheduler: Missing parents: List()\n",
      "23/08/20 11:42:05 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[36] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/08/20 11:42:05 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 16.0 KiB, free 433.6 MiB)\n",
      "23/08/20 11:42:05 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.6 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/20 11:42:05 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 127.0.0.1:50841 (size: 8.1 KiB, free: 434.3 MiB)\n",
      "23/08/20 11:42:05 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\n",
      "23/08/20 11:42:05 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[36] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/08/20 11:42:05 INFO YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "23/08/20 11:42:05 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (localhost, executor 1, partition 0, PROCESS_LOCAL, 4914 bytes) taskResourceAssignments Map()\n",
      "23/08/20 11:42:05 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:50849 (size: 8.1 KiB, free: 434.4 MiB)\n",
      "23/08/20 11:42:05 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:50849 (size: 34.9 KiB, free: 434.3 MiB)\n",
      "23/08/20 11:42:06 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 357 ms on localhost (executor 1) (1/1)\n",
      "23/08/20 11:42:06 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "23/08/20 11:42:06 INFO DAGScheduler: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.388 s\n",
      "23/08/20 11:42:06 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/08/20 11:42:06 INFO DAGScheduler: running: Set()\n",
      "23/08/20 11:42:06 INFO DAGScheduler: waiting: Set()\n",
      "23/08/20 11:42:06 INFO DAGScheduler: failed: Set()\n",
      "23/08/20 11:42:06 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "23/08/20 11:42:06 INFO DAGScheduler: Got job 9 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/08/20 11:42:06 INFO DAGScheduler: Final stage: ResultStage 11 (count at NativeMethodAccessorImpl.java:0)\n",
      "23/08/20 11:42:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\n",
      "23/08/20 11:42:06 INFO DAGScheduler: Missing parents: List()\n",
      "23/08/20 11:42:06 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[39] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/08/20 11:42:06 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 11.1 KiB, free 433.6 MiB)\n",
      "23/08/20 11:42:06 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.6 MiB)\n",
      "23/08/20 11:42:06 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 127.0.0.1:50841 (size: 5.5 KiB, free: 434.3 MiB)\n",
      "23/08/20 11:42:06 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\n",
      "23/08/20 11:42:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[39] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/08/20 11:42:06 INFO YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\n",
      "23/08/20 11:42:06 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 10) (localhost, executor 2, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\n",
      "23/08/20 11:42:06 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:50853 (size: 5.5 KiB, free: 434.3 MiB)\n",
      "23/08/20 11:42:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 127.0.0.1:50851\n",
      "23/08/20 11:42:06 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 10) in 86 ms on localhost (executor 2) (1/1)\n",
      "23/08/20 11:42:06 INFO YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "23/08/20 11:42:06 INFO DAGScheduler: ResultStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.102 s\n",
      "23/08/20 11:42:06 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/08/20 11:42:06 INFO YarnScheduler: Killing all running tasks in stage 11: Stage finished\n",
      "23/08/20 11:42:06 INFO DAGScheduler: Job 9 finished: count at NativeMethodAccessorImpl.java:0, took 0.110656 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "256"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = spark.read.csv(\"hdfs://localhost:9000/data/2015-summary.csv\", header=True, inferSchema=True)\n",
    "data.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T02:42:06.238246Z",
     "start_time": "2023-08-20T02:41:59.808698Z"
    }
   },
   "id": "b9c7e41e32998a2e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: int]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.repartition(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T22:25:15.109235Z",
     "start_time": "2023-08-17T22:25:15.094627Z"
    }
   },
   "id": "dab85489059daeb1"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/18 07:25:22 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/08/18 07:25:22 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/08/18 07:25:22 INFO FileSourceStrategy: Output Data Schema: struct<DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: int ... 1 more fields>\n",
      "23/08/18 07:25:22 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 201.0 KiB, free 433.4 MiB)\n",
      "23/08/18 07:25:22 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 433.4 MiB)\n",
      "23/08/18 07:25:22 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:50387 (size: 34.9 KiB, free: 434.2 MiB)\n",
      "23/08/18 07:25:22 INFO SparkContext: Created broadcast 7 from take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/3540522002.py:1\n",
      "23/08/18 07:25:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/08/18 07:25:22 INFO SparkContext: Starting job: take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/3540522002.py:1\n",
      "23/08/18 07:25:22 INFO DAGScheduler: Got job 4 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/3540522002.py:1) with 1 output partitions\n",
      "23/08/18 07:25:22 INFO DAGScheduler: Final stage: ResultStage 5 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/3540522002.py:1)\n",
      "23/08/18 07:25:22 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/08/18 07:25:22 INFO DAGScheduler: Missing parents: List()\n",
      "23/08/18 07:25:22 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[19] at take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/3540522002.py:1), which has no missing parents\n",
      "23/08/18 07:25:22 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 10.5 KiB, free 433.4 MiB)\n",
      "23/08/18 07:25:22 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 433.4 MiB)\n",
      "23/08/18 07:25:22 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 127.0.0.1:50387 (size: 5.8 KiB, free: 434.2 MiB)\n",
      "23/08/18 07:25:22 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\n",
      "23/08/18 07:25:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[19] at take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/3540522002.py:1) (first 15 tasks are for partitions Vector(0))\n",
      "23/08/18 07:25:22 INFO YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "23/08/18 07:25:22 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (localhost, executor 2, partition 0, PROCESS_LOCAL, 4925 bytes) taskResourceAssignments Map()\n",
      "23/08/18 07:25:22 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:50397 (size: 5.8 KiB, free: 434.3 MiB)\n",
      "23/08/18 07:25:22 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:50397 (size: 34.9 KiB, free: 434.3 MiB)\n",
      "23/08/18 07:25:22 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 315 ms on localhost (executor 2) (1/1)\n",
      "23/08/18 07:25:22 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "23/08/18 07:25:22 INFO DAGScheduler: ResultStage 5 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/3540522002.py:1) finished in 0.333 s\n",
      "23/08/18 07:25:22 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/08/18 07:25:22 INFO YarnScheduler: Killing all running tasks in stage 5: Stage finished\n",
      "23/08/18 07:25:22 INFO DAGScheduler: Job 4 finished: take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/3540522002.py:1, took 0.342214 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataframe에서 csv파일을 읽어 로컬 배열이나 리스트 형태로 변환\n",
    "\n",
    "data.take(3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T22:25:22.616632Z",
     "start_time": "2023-08-17T22:25:22.151764Z"
    }
   },
   "id": "e7efe31226ddef65"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/18 07:25:41 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/08/18 07:25:41 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/08/18 07:25:41 INFO FileSourceStrategy: Output Data Schema: struct<DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: int ... 1 more fields>\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#19 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#19 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=63]\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#17,ORIGIN_COUNTRY_NAME#18,count#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://localhost:9000/data/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "data.sort('count').explain()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T22:25:42.015199Z",
     "start_time": "2023-08-17T22:25:41.930415Z"
    }
   },
   "id": "136f1f9cc37f5d21"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "## Spark SQL : DataFrame -> add table or view and select\n",
    "data.createOrReplaceTempView(\"flight_data_2015\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T02:43:03.058586Z",
     "start_time": "2023-08-20T02:43:02.994284Z"
    }
   },
   "id": "ee1a39e43d11b648"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/20 11:43:03 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/08/20 11:43:03 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/08/20 11:43:03 INFO FileSourceStrategy: Output Data Schema: struct<DEST_COUNTRY_NAME: string>\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[dest_country_name#51], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(dest_country_name#51, 200), ENSURE_REQUIREMENTS, [plan_id=153]\n",
      "      +- HashAggregate(keys=[dest_country_name#51], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#51] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://localhost:9000/data/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select dest_country_name, count(1)\n",
    "from flight_data_2015\n",
    "group by dest_country_name\n",
    "''').explain()\n",
    "\n",
    "## SPARK SQL, Dataframe 실행 계획은 동일"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T02:43:03.713553Z",
     "start_time": "2023-08-20T02:43:03.654954Z"
    }
   },
   "id": "c2ad1931fee1ac4d"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/20 11:45:36 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/08/20 11:45:36 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/08/20 11:45:36 INFO FileSourceStrategy: Output Data Schema: struct<count: int>\n",
      "23/08/20 11:45:36 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 201.0 KiB, free 432.4 MiB)\n",
      "23/08/20 11:45:36 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 432.3 MiB)\n",
      "23/08/20 11:45:36 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 127.0.0.1:50841 (size: 34.9 KiB, free: 434.1 MiB)\n",
      "23/08/20 11:45:36 INFO SparkContext: Created broadcast 23 from take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/784141438.py:1\n",
      "23/08/20 11:45:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/08/20 11:45:36 INFO DAGScheduler: Registering RDD 63 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/784141438.py:1) as input to shuffle 4\n",
      "23/08/20 11:45:36 INFO DAGScheduler: Got map stage job 16 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/784141438.py:1) with 1 output partitions\n",
      "23/08/20 11:45:36 INFO DAGScheduler: Final stage: ShuffleMapStage 20 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/784141438.py:1)\n",
      "23/08/20 11:45:36 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/08/20 11:45:36 INFO DAGScheduler: Missing parents: List()\n",
      "23/08/20 11:45:36 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[63] at take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/784141438.py:1), which has no missing parents\n",
      "23/08/20 11:45:36 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 16.8 KiB, free 432.3 MiB)\n",
      "23/08/20 11:45:36 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 432.3 MiB)\n",
      "23/08/20 11:45:36 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 127.0.0.1:50841 (size: 8.4 KiB, free: 434.0 MiB)\n",
      "23/08/20 11:45:36 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\n",
      "23/08/20 11:45:36 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[63] at take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/784141438.py:1) (first 15 tasks are for partitions Vector(0))\n",
      "23/08/20 11:45:36 INFO YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\n",
      "23/08/20 11:45:36 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 17) (localhost, executor 2, partition 0, PROCESS_LOCAL, 4914 bytes) taskResourceAssignments Map()\n",
      "23/08/20 11:45:36 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on localhost:50853 (size: 8.4 KiB, free: 434.3 MiB)\n",
      "23/08/20 11:45:36 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on localhost:50853 (size: 34.9 KiB, free: 434.3 MiB)\n",
      "23/08/20 11:45:36 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 17) in 292 ms on localhost (executor 2) (1/1)\n",
      "23/08/20 11:45:36 INFO YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "23/08/20 11:45:36 INFO DAGScheduler: ShuffleMapStage 20 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/784141438.py:1) finished in 0.318 s\n",
      "23/08/20 11:45:36 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/08/20 11:45:36 INFO DAGScheduler: running: Set()\n",
      "23/08/20 11:45:36 INFO DAGScheduler: waiting: Set()\n",
      "23/08/20 11:45:36 INFO DAGScheduler: failed: Set()\n",
      "23/08/20 11:45:36 INFO SparkContext: Starting job: take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/784141438.py:1\n",
      "23/08/20 11:45:36 INFO DAGScheduler: Got job 17 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/784141438.py:1) with 1 output partitions\n",
      "23/08/20 11:45:36 INFO DAGScheduler: Final stage: ResultStage 22 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/784141438.py:1)\n",
      "23/08/20 11:45:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\n",
      "23/08/20 11:45:36 INFO DAGScheduler: Missing parents: List()\n",
      "23/08/20 11:45:36 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[66] at take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/784141438.py:1), which has no missing parents\n",
      "23/08/20 11:45:36 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 11.7 KiB, free 432.3 MiB)\n",
      "23/08/20 11:45:36 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 432.3 MiB)\n",
      "23/08/20 11:45:36 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 127.0.0.1:50841 (size: 5.7 KiB, free: 434.0 MiB)\n",
      "23/08/20 11:45:36 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\n",
      "23/08/20 11:45:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[66] at take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/784141438.py:1) (first 15 tasks are for partitions Vector(0))\n",
      "23/08/20 11:45:36 INFO YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\n",
      "23/08/20 11:45:36 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 18) (localhost, executor 2, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\n",
      "23/08/20 11:45:36 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on localhost:50853 (size: 5.7 KiB, free: 434.3 MiB)\n",
      "23/08/20 11:45:36 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 127.0.0.1:50851\n",
      "23/08/20 11:45:36 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 18) in 109 ms on localhost (executor 2) (1/1)\n",
      "23/08/20 11:45:36 INFO YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "23/08/20 11:45:36 INFO DAGScheduler: ResultStage 22 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/784141438.py:1) finished in 0.121 s\n",
      "23/08/20 11:45:36 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/08/20 11:45:36 INFO YarnScheduler: Killing all running tasks in stage 22: Stage finished\n",
      "23/08/20 11:45:36 INFO DAGScheduler: Job 17 finished: take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/784141438.py:1, took 0.133400 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(max_count=370002)]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select max(t1.count) as max_count\n",
    "    from flight_data_2015 as t1\n",
    "\"\"\").take(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T02:45:36.827311Z",
     "start_time": "2023-08-20T02:45:36.213818Z"
    }
   },
   "id": "faef36acba51ae96"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/20 11:46:32 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/08/20 11:46:32 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/08/20 11:46:32 INFO FileSourceStrategy: Output Data Schema: struct<count: int>\n",
      "23/08/20 11:46:32 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 201.0 KiB, free 431.9 MiB)\n",
      "23/08/20 11:46:32 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 431.8 MiB)\n",
      "23/08/20 11:46:32 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 127.0.0.1:50841 (size: 34.9 KiB, free: 434.0 MiB)\n",
      "23/08/20 11:46:32 INFO SparkContext: Created broadcast 28 from take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/4170910463.py:2\n",
      "23/08/20 11:46:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/08/20 11:46:32 INFO DAGScheduler: Registering RDD 73 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/4170910463.py:2) as input to shuffle 5\n",
      "23/08/20 11:46:32 INFO DAGScheduler: Got map stage job 19 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/4170910463.py:2) with 1 output partitions\n",
      "23/08/20 11:46:32 INFO DAGScheduler: Final stage: ShuffleMapStage 24 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/4170910463.py:2)\n",
      "23/08/20 11:46:32 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/08/20 11:46:32 INFO DAGScheduler: Missing parents: List()\n",
      "23/08/20 11:46:32 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[73] at take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/4170910463.py:2), which has no missing parents\n",
      "23/08/20 11:46:32 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 16.8 KiB, free 431.8 MiB)\n",
      "23/08/20 11:46:32 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 431.8 MiB)\n",
      "23/08/20 11:46:32 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 127.0.0.1:50841 (size: 8.4 KiB, free: 434.0 MiB)\n",
      "23/08/20 11:46:32 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\n",
      "23/08/20 11:46:32 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[73] at take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/4170910463.py:2) (first 15 tasks are for partitions Vector(0))\n",
      "23/08/20 11:46:32 INFO YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\n",
      "23/08/20 11:46:32 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 20) (localhost, executor 2, partition 0, PROCESS_LOCAL, 4914 bytes) taskResourceAssignments Map()\n",
      "23/08/20 11:46:32 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on localhost:50853 (size: 8.4 KiB, free: 434.3 MiB)\n",
      "23/08/20 11:46:32 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on localhost:50853 (size: 34.9 KiB, free: 434.2 MiB)\n",
      "23/08/20 11:46:32 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 20) in 161 ms on localhost (executor 2) (1/1)\n",
      "23/08/20 11:46:32 INFO YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "23/08/20 11:46:32 INFO DAGScheduler: ShuffleMapStage 24 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/4170910463.py:2) finished in 0.184 s\n",
      "23/08/20 11:46:32 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/08/20 11:46:32 INFO DAGScheduler: running: Set()\n",
      "23/08/20 11:46:32 INFO DAGScheduler: waiting: Set()\n",
      "23/08/20 11:46:32 INFO DAGScheduler: failed: Set()\n",
      "23/08/20 11:46:32 INFO SparkContext: Starting job: take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/4170910463.py:2\n",
      "23/08/20 11:46:32 INFO DAGScheduler: Got job 20 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/4170910463.py:2) with 1 output partitions\n",
      "23/08/20 11:46:32 INFO DAGScheduler: Final stage: ResultStage 26 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/4170910463.py:2)\n",
      "23/08/20 11:46:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\n",
      "23/08/20 11:46:32 INFO DAGScheduler: Missing parents: List()\n",
      "23/08/20 11:46:32 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[76] at take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/4170910463.py:2), which has no missing parents\n",
      "23/08/20 11:46:32 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 11.7 KiB, free 431.8 MiB)\n",
      "23/08/20 11:46:32 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 431.8 MiB)\n",
      "23/08/20 11:46:32 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 127.0.0.1:50841 (size: 5.7 KiB, free: 434.0 MiB)\n",
      "23/08/20 11:46:32 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\n",
      "23/08/20 11:46:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[76] at take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/4170910463.py:2) (first 15 tasks are for partitions Vector(0))\n",
      "23/08/20 11:46:32 INFO YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\n",
      "23/08/20 11:46:32 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 21) (localhost, executor 2, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\n",
      "23/08/20 11:46:32 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on localhost:50853 (size: 5.7 KiB, free: 434.2 MiB)\n",
      "23/08/20 11:46:32 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 127.0.0.1:50851\n",
      "23/08/20 11:46:32 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 21) in 42 ms on localhost (executor 2) (1/1)\n",
      "23/08/20 11:46:32 INFO YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
      "23/08/20 11:46:32 INFO DAGScheduler: ResultStage 26 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/4170910463.py:2) finished in 0.052 s\n",
      "23/08/20 11:46:32 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/08/20 11:46:32 INFO YarnScheduler: Killing all running tasks in stage 26: Stage finished\n",
      "23/08/20 11:46:32 INFO DAGScheduler: Job 20 finished: take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_9620/4170910463.py:2, took 0.058715 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(max(count)=370002)]"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "data.select(f.max(\"count\")).take(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T02:46:32.650584Z",
     "start_time": "2023-08-20T02:46:32.260808Z"
    }
   },
   "id": "885085fd14ab1353"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/20 11:53:39 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/08/20 11:53:39 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/08/20 11:53:39 INFO FileSourceStrategy: Output Data Schema: struct<DEST_COUNTRY_NAME: string, count: int>\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- TakeOrderedAndProject(limit=5, orderBy=[destination_total#203L DESC NULLS LAST], output=[dest_country_name#51,destination_total#203L])\n",
      "   +- HashAggregate(keys=[dest_country_name#51], functions=[sum(count#53)])\n",
      "      +- Exchange hashpartitioning(dest_country_name#51, 200), ENSURE_REQUIREMENTS, [plan_id=515]\n",
      "         +- HashAggregate(keys=[dest_country_name#51], functions=[partial_sum(count#53)])\n",
      "            +- FileScan csv [DEST_COUNTRY_NAME#51,count#53] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://localhost:9000/data/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "maxSql = spark.sql(\"\"\"\n",
    "                   select t1.dest_country_name, sum(t1.count) as destination_total\n",
    "                   from flight_data_2015 as t1\n",
    "                   group by t1.dest_country_name\n",
    "                   order by sum(t1.count) desc\n",
    "                   limit 5\n",
    "                   \"\"\")\n",
    "\n",
    "maxSql.explain()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T02:53:39.168955Z",
     "start_time": "2023-08-20T02:53:39.109987Z"
    }
   },
   "id": "11c4a4457b6aa91a"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/20 11:53:29 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/08/20 11:53:29 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/08/20 11:53:29 INFO FileSourceStrategy: Output Data Schema: struct<DEST_COUNTRY_NAME: string, count: int>\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- TakeOrderedAndProject(limit=5, orderBy=[destination_total#198L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#51,destination_total#198L])\n",
      "   +- HashAggregate(keys=[DEST_COUNTRY_NAME#51], functions=[sum(count#53)])\n",
      "      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#51, 200), ENSURE_REQUIREMENTS, [plan_id=498]\n",
      "         +- HashAggregate(keys=[DEST_COUNTRY_NAME#51], functions=[partial_sum(count#53)])\n",
      "            +- FileScan csv [DEST_COUNTRY_NAME#51,count#53] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://localhost:9000/data/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "data.groupBy(\"DEST_COUNTRY_NAME\") \\\n",
    "    .sum(\"count\") \\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\") \\\n",
    "    .sort(f.desc(\"destination_total\")).limit(5) \\\n",
    "    .explain()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T02:53:29.317567Z",
     "start_time": "2023-08-20T02:53:29.255425Z"
    }
   },
   "id": "1ae3cd23557318a9"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "static_df = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .option('inferSchema', 'true') \\\n",
    "    .load('hdfs://localhost:9000/data/by-day/*.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T23:41:47.638131Z",
     "start_time": "2023-08-20T23:41:24.686441Z"
    }
   },
   "id": "1c1af1689932a16f"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "static_df.repartition(10)\n",
    "static_df.createOrReplaceTempView('retail_data')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T23:41:49.600118Z",
     "start_time": "2023-08-20T23:41:49.513126Z"
    }
   },
   "id": "9e38e3d2cd6e3bb4"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "static_schema = static_df.schema"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T23:41:50.635090Z",
     "start_time": "2023-08-20T23:41:50.628976Z"
    }
   },
   "id": "a2a6ae5a0d74d010"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "StructType([StructField('InvoiceNo', StringType(), True), StructField('StockCode', StringType(), True), StructField('Description', StringType(), True), StructField('Quantity', IntegerType(), True), StructField('InvoiceDate', TimestampType(), True), StructField('UnitPrice', DoubleType(), True), StructField('CustomerID', DoubleType(), True), StructField('Country', StringType(), True)])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_schema"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T23:41:51.627350Z",
     "start_time": "2023-08-20T23:41:51.602728Z"
    }
   },
   "id": "e620848d3819a0c7"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 101:============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   17450.0|{2011-09-20 09:00...|          71601.44|\n",
      "|   18102.0|{2011-09-15 09:00...|31661.540000000005|\n",
      "|   18102.0|{2011-10-21 09:00...|          29693.82|\n",
      "|   14646.0|{2011-10-20 09:00...|28148.039999999994|\n",
      "|   18102.0|{2010-12-07 09:00...|          25920.37|\n",
      "|   12415.0|{2011-06-15 09:00...| 23426.81000000001|\n",
      "|   15749.0|{2011-01-11 09:00...|           22998.4|\n",
      "|   18102.0|{2011-10-03 09:00...|          22429.69|\n",
      "|   12415.0|{2011-08-17 09:00...|21880.439999999995|\n",
      "|   14646.0|{2011-08-11 09:00...| 19150.65999999999|\n",
      "|   12931.0|{2011-08-04 09:00...|19045.480000000003|\n",
      "|   17949.0|{2011-06-30 09:00...|18854.780000000002|\n",
      "|   14646.0|{2011-02-21 09:00...|18279.479999999996|\n",
      "|   14646.0|{2011-03-29 09:00...|           18247.5|\n",
      "|   16684.0|{2011-10-05 09:00...|          18047.42|\n",
      "|   14156.0|{2011-01-14 09:00...|16774.719999999998|\n",
      "|   12415.0|{2011-03-03 09:00...|          16558.14|\n",
      "|   18102.0|{2011-06-09 09:00...|           16488.0|\n",
      "|   14646.0|{2011-05-12 09:00...|16478.460000000006|\n",
      "|   12415.0|{2011-10-05 09:00...|16471.770000000004|\n",
      "+----------+--------------------+------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "static_df.selectExpr(\"CustomerID\",\n",
    "                     \"(UnitPrice * Quantity) as total_cost\",\n",
    "                     \"InvoiceDate\"\n",
    "                     ).groupBy(f.col(\"CustomerId\"),\n",
    "                               f.window(f.col('InvoiceDate'), \"1 day\")).sum(\"total_cost\") \\\n",
    "    .where(f.col(\"CustomerID\").isNotNull()) \\\n",
    "    .sort(f.desc(\"sum(total_cost)\")) \\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T03:45:17.040132Z",
     "start_time": "2023-08-20T03:45:14.387167Z"
    }
   },
   "id": "43ba09c5321d9d89"
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 95:=======================================>                 (7 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|        total_cost|\n",
      "+----------+--------------------+------------------+\n",
      "|   17450.0|{2011-09-20 09:00...|          71601.44|\n",
      "|   18102.0|{2011-09-15 09:00...|31661.540000000005|\n",
      "|   18102.0|{2011-10-21 09:00...|          29693.82|\n",
      "|   14646.0|{2011-10-20 09:00...|28148.039999999994|\n",
      "|   18102.0|{2010-12-07 09:00...|          25920.37|\n",
      "|   12415.0|{2011-06-15 09:00...| 23426.81000000001|\n",
      "|   15749.0|{2011-01-11 09:00...|           22998.4|\n",
      "|   18102.0|{2011-10-03 09:00...|          22429.69|\n",
      "|   12415.0|{2011-08-17 09:00...|21880.439999999995|\n",
      "|   14646.0|{2011-08-11 09:00...| 19150.65999999999|\n",
      "|   12931.0|{2011-08-04 09:00...|19045.480000000003|\n",
      "|   17949.0|{2011-06-30 09:00...|18854.780000000002|\n",
      "|   14646.0|{2011-02-21 09:00...|18279.479999999996|\n",
      "|   14646.0|{2011-03-29 09:00...|           18247.5|\n",
      "|   16684.0|{2011-10-05 09:00...|          18047.42|\n",
      "|   14156.0|{2011-01-14 09:00...|16774.719999999998|\n",
      "|   12415.0|{2011-03-03 09:00...|          16558.14|\n",
      "|   18102.0|{2011-06-09 09:00...|           16488.0|\n",
      "|   14646.0|{2011-05-12 09:00...|16478.460000000006|\n",
      "|   12415.0|{2011-10-05 09:00...|16471.770000000004|\n",
      "+----------+--------------------+------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  t1.CustomerId,\n",
    "  window(t1.InvoiceDate, '1 day') AS window,\n",
    "  SUM(t1.UnitPrice * t1.Quantity) AS total_cost\n",
    "FROM\n",
    "  retail_data as t1\n",
    "  where t1.CustomerId is not null\n",
    "GROUP BY CustomerId, window(InvoiceDate, '1 day')\n",
    "order by total_cost desc\n",
    "\"\"\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T03:39:34.147111Z",
     "start_time": "2023-08-20T03:39:31.247757Z"
    }
   },
   "id": "b0ef2e678c52b3b6"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 111:==================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|              window|CustomerId|\n",
      "+--------------------+----------+\n",
      "|{2011-10-31 09:00...|   17450.0|\n",
      "|{2011-11-29 09:00...|   17450.0|\n",
      "|{2011-11-28 09:00...|   17450.0|\n",
      "|{2011-09-21 09:00...|   17450.0|\n",
      "|{2011-11-13 09:00...|   17450.0|\n",
      "|{2011-05-24 09:00...|   17450.0|\n",
      "|{2011-09-30 09:00...|   17450.0|\n",
      "|{2011-07-14 09:00...|   17450.0|\n",
      "|{2011-09-20 09:00...|   17450.0|\n",
      "|{2011-06-21 09:00...|   17450.0|\n",
      "|{2011-01-10 09:00...|   17450.0|\n",
      "|{2011-06-16 09:00...|   17450.0|\n",
      "|{2011-08-23 09:00...|   17450.0|\n",
      "|{2011-01-11 09:00...|   17450.0|\n",
      "|{2011-07-28 09:00...|   17450.0|\n",
      "|{2011-03-09 09:00...|   17450.0|\n",
      "|{2011-08-31 09:00...|   17450.0|\n",
      "|{2011-05-31 09:00...|   17450.0|\n",
      "|{2011-11-21 09:00...|   17450.0|\n",
      "|{2010-12-07 09:00...|   17450.0|\n",
      "+--------------------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  window(t1.InvoiceDate, '1 day') AS window,\n",
    "  t1.CustomerId\n",
    "FROM\n",
    "  retail_data as t1\n",
    "  where t1.CustomerId = '17450'\n",
    "  group by CustomerId, window(t1.InvoiceDate, '1 day') \n",
    "\"\"\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T03:48:19.502231Z",
     "start_time": "2023-08-20T03:48:16.895121Z"
    }
   },
   "id": "3a5cca2bb1d52bfb"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 121:=================================>                      (6 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "###  Spark Stream ###\n",
    "\n",
    "spark.sql(\"SELECT count(1) from retail_data\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T03:55:05.965198Z",
     "start_time": "2023-08-20T03:55:04.488993Z"
    }
   },
   "id": "76a056054b2811e3"
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 163:=======================================>                (7 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+\n",
      "|   581587|    22556|PLASTERS IN TIN C...|      12|2011-12-09 12:50:00|     1.65|   12680.0| France|\n",
      "|   581587|    22631|CIRCUS PARADE LUN...|      12|2011-12-09 12:50:00|     1.95|   12680.0| France|\n",
      "|   581587|    22726|ALARM CLOCK BAKEL...|       4|2011-12-09 12:50:00|     3.75|   12680.0| France|\n",
      "|   581587|    22555|PLASTERS IN TIN S...|      12|2011-12-09 12:50:00|     1.65|   12680.0| France|\n",
      "|   581587|    22728|ALARM CLOCK BAKEL...|       4|2011-12-09 12:50:00|     3.75|   12680.0| France|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## add 2011-12-10 on hdfs\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select * from retail_data as t1\n",
    "    order by t1.InvoiceDate desc\n",
    "    limit 5\n",
    "\"\"\").show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T04:02:14.808463Z",
     "start_time": "2023-08-20T04:02:12.107931Z"
    }
   },
   "id": "40f14b1cd9960159"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 스트리밍 Action\n",
    "\n",
    "# 스트리밍 액션은 어딘가에 데이터를 채워 넣어야 하므로 일반적인 정적 액션과는 다른 특성을 가짐\n",
    "# 본 예제에서 사용할 스트리밍 액션은 트리거가 실행된 다음 데이터를 갱신하게 될 인메모리 테이블에 데이터를 저장함\n",
    "# 스파크는 이전 집계값보다 더 큰 값이 발생한 경우에만 인메모리 테이블을 갱신함\n",
    "# 스트림이 시작되면 쿼리 실행 결과가 어떠한 형태로 인메모리 테이블에 기록되는지 확인 가능"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2e61470c982fbdc"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "static_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"hdfs://localhost:9000/data/by-day/*.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T00:52:34.351461Z",
     "start_time": "2023-08-21T00:52:12.319369Z"
    }
   },
   "id": "89234d8433c3706c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "static_schema = static_df.schema"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T00:53:15.638492Z",
     "start_time": "2023-08-21T00:53:15.619438Z"
    }
   },
   "id": "f6c2e1d2e92f31fa"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "per_df = static_df.na.fill(0).withColumn(\"day_of_week\", f.date_format(f.col(\"InvoiceDate\"), \"EEEE\")) \\\n",
    "    .coalesce(5)  ## coalesce는 파티션 수를 줄이기만 가능"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T00:54:44.532634Z",
     "start_time": "2023-08-21T00:54:44.416134Z"
    }
   },
   "id": "658e5ab910b9b633"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|day_of_week|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|     Monday|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|     Monday|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "per_df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T00:55:02.731737Z",
     "start_time": "2023-08-21T00:55:01.965942Z"
    }
   },
   "id": "7fc62833aa4297bc"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_df.rdd.getNumPartitions()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T00:55:58.912761Z",
     "start_time": "2023-08-21T00:55:58.722268Z"
    }
   },
   "id": "86d31f9a27a37bcf"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|        InvoiceNo|\n",
      "+-------+-----------------+\n",
      "|  count|           541909|\n",
      "|   mean| 559965.752026781|\n",
      "| stddev|13428.41728079509|\n",
      "|    min|           536365|\n",
      "|    max|          C581569|\n",
      "+-------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "per_df.select(f.col(\"InvoiceNo\")).describe().show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T00:57:18.002662Z",
     "start_time": "2023-08-21T00:57:15.105453Z"
    }
   },
   "id": "a98f2459e5ae0345"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "## split test / train\n",
    "\n",
    "train_df = per_df.where(f.col(\"InvoiceNo\") < \"559965\")\n",
    "test_df = per_df.where(f.col(\"InvoiceNo\") > \"559965\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T01:01:37.136684Z",
     "start_time": "2023-08-21T01:01:37.100429Z"
    }
   },
   "id": "37b54aa32b6fd910"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-24991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(train_df.count() - test_df.count())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T01:03:24.282151Z",
     "start_time": "2023-08-21T01:03:20.048016Z"
    }
   },
   "id": "250443b7989260e8"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "### 전처리\n",
    "#StringIndexer로 각 요일을 수치형으로 변환\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer().setInputCol(\"day_of_week\").setOutputCol(\"day_of_week_index\") # Mon -> 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T01:05:15.830824Z",
     "start_time": "2023-08-21T01:05:15.789595Z"
    }
   },
   "id": "306b087973c2dcb7"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "#OneHotEncoder로 index -> binary vector로 변환\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder().setInputCol(\"day_of_week_index\").setOutputCol(\"day_of_week_encoded\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T01:06:24.586271Z",
     "start_time": "2023-08-21T01:06:24.548710Z"
    }
   },
   "id": "ea9302ea49cb3cc9"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vector_asb = VectorAssembler().setInputCols(['UnitPrice', 'Quantity', 'day_of_week_encoded']).setOutputCol('features')\n",
    "# 스파크의 모든 머신러닝 알고리즘은 수치형 벡터 타입을 입력으로 사용\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "trans_pipeline = Pipeline().setStages([indexer, encoder, vector_asb]) # pipeline -> indexer -> encoder -> vector_asb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T01:07:49.557109Z",
     "start_time": "2023-08-21T01:07:49.522024Z"
    }
   },
   "id": "4e99277c4c7a5914"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fit_pipeline = trans_pipeline.fit(train_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T01:08:37.422057Z",
     "start_time": "2023-08-21T01:08:32.828715Z"
    }
   },
   "id": "4ca72daa84054695"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "trans_train = fit_pipeline.transform(train_df).cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T01:09:05.561652Z",
     "start_time": "2023-08-21T01:09:05.256465Z"
    }
   },
   "id": "47b4b7d825f21b82"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------------+-------------------+--------------------+\n",
      "|InvoiceNo|day_of_week|day_of_week_index|day_of_week_encoded|            features|\n",
      "+---------+-----------+-----------------+-------------------+--------------------+\n",
      "|   537226|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[2.95,...|\n",
      "|   537226|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[2.1,8...|\n",
      "|   537226|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[5.95,...|\n",
      "|   537226|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[1.65,...|\n",
      "|   537226|     Monday|              2.0|      (5,[2],[1.0])|(7,[0,1,4],[0.42,...|\n",
      "+---------+-----------+-----------------+-------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "trans_train.select(\"InvoiceNo\", \"day_of_week\", \"day_of_week_index\", \"day_of_week_encoded\", \"features\").show(5)\n",
    "\n",
    "# 모델의 하이퍼파라미터를 튜닝할 때, 캐싱을 사용하면 중간 변환된 데이터셋의 복사본을 메모리에 저장하므로 빠르게 반복적으로 수행 가능"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T01:13:48.673258Z",
     "start_time": "2023-08-21T01:13:48.173190Z"
    }
   },
   "id": "807bc02f4e41b044"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "KMeansModel: uid=KMeans_633452abd5c5, k=10, distanceMeasure=euclidean, numFeatures=7"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans,KMeansModel\n",
    "\n",
    "kmeans= KMeans().setK(10).setSeed(1)\n",
    "model = kmeans.fit(trans_train)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T01:14:16.357236Z",
     "start_time": "2023-08-21T01:14:11.239991Z"
    }
   },
   "id": "5d3e4c9084719733"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot  as plt\n",
    "import seaborn as sns\n",
    "from pyspark.ml.feature import PCA\n",
    "import pandas as pd\n",
    "\n",
    "transformed_data = model.transform(trans_train)\n",
    "\n",
    "num_dimensions = 2  # You can also use 3 dimensions\n",
    "pca = PCA(k=num_dimensions, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(transformed_data)\n",
    "transformed_data = pca_model.transform(transformed_data)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame for plotting\n",
    "pandas_df = transformed_data.select(\"pca_features\", \"prediction\").toPandas()\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pandas_df[\"pca_features\"].apply(lambda x: x[0]), pandas_df[\"pca_features\"].apply(lambda x: x[1]), c=pandas_df[\"prediction\"], cmap=\"rainbow\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T01:37:14.820901Z",
     "start_time": "2023-08-21T01:37:07.124673Z"
    }
   },
   "id": "7c063ed6b2e3826c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5e82d7de71dfee0e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
