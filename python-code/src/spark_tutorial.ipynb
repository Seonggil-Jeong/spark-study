{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-17T22:21:30.840621Z",
     "start_time": "2023-08-17T22:21:01.731207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/18 07:21:04 WARN Utils: Your hostname, SeongGils-MacBook-Pro-2.local resolves to a loopback address: 127.0.0.1; using 192.168.219.137 instead (on interface en0)\n",
      "23/08/18 07:21:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/18 07:21:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/08/18 07:21:08 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, transform\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkTutorial\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/18 07:24:52 INFO InMemoryFileIndex: It took 106 ms to list leaf files for 1 paths.\n",
      "23/08/18 07:24:53 INFO InMemoryFileIndex: It took 11 ms to list leaf files for 1 paths.\n",
      "23/08/18 07:24:55 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/08/18 07:24:55 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "23/08/18 07:24:55 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "23/08/18 07:24:56 INFO CodeGenerator: Code generated in 258.136166 ms\n",
      "23/08/18 07:24:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.1 KiB, free 434.2 MiB)\n",
      "23/08/18 07:24:57 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 434.2 MiB)\n",
      "23/08/18 07:24:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:50387 (size: 34.9 KiB, free: 434.4 MiB)\n",
      "23/08/18 07:24:57 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n",
      "23/08/18 07:24:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/08/18 07:24:57 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "23/08/18 07:24:57 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/08/18 07:24:57 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n",
      "23/08/18 07:24:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/08/18 07:24:57 INFO DAGScheduler: Missing parents: List()\n",
      "23/08/18 07:24:57 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/08/18 07:24:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.8 KiB, free 434.2 MiB)\n",
      "23/08/18 07:24:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.2 MiB)\n",
      "23/08/18 07:24:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:50387 (size: 5.9 KiB, free: 434.4 MiB)\n",
      "23/08/18 07:24:57 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "23/08/18 07:24:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/08/18 07:24:57 INFO YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "23/08/18 07:24:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (localhost, executor 1, partition 0, PROCESS_LOCAL, 4925 bytes) taskResourceAssignments Map()\n",
      "23/08/18 07:24:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:50394 (size: 5.9 KiB, free: 434.4 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/18 07:24:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:50394 (size: 34.9 KiB, free: 434.4 MiB)\n",
      "23/08/18 07:25:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2814 ms on localhost (executor 1) (1/1)\n",
      "23/08/18 07:25:00 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "23/08/18 07:25:00 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 2.964 s\n",
      "23/08/18 07:25:00 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/08/18 07:25:00 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished\n",
      "23/08/18 07:25:00 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 3.010472 s\n",
      "23/08/18 07:25:00 INFO CodeGenerator: Code generated in 39.857625 ms\n",
      "23/08/18 07:25:00 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/08/18 07:25:00 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/08/18 07:25:00 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "23/08/18 07:25:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.1 KiB, free 434.0 MiB)\n",
      "23/08/18 07:25:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 433.9 MiB)\n",
      "23/08/18 07:25:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:50387 (size: 34.9 KiB, free: 434.3 MiB)\n",
      "23/08/18 07:25:00 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\n",
      "23/08/18 07:25:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/18 07:25:00 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "23/08/18 07:25:00 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/08/18 07:25:00 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
      "23/08/18 07:25:00 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/08/18 07:25:00 INFO DAGScheduler: Missing parents: List()\n",
      "23/08/18 07:25:00 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/08/18 07:25:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 25.4 KiB, free 433.9 MiB)\n",
      "23/08/18 07:25:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 11.9 KiB, free 433.9 MiB)\n",
      "23/08/18 07:25:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:50387 (size: 11.9 KiB, free: 434.3 MiB)\n",
      "23/08/18 07:25:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\n",
      "23/08/18 07:25:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/08/18 07:25:00 INFO YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "23/08/18 07:25:00 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (localhost, executor 2, partition 0, PROCESS_LOCAL, 4925 bytes) taskResourceAssignments Map()\n",
      "23/08/18 07:25:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:50397 (size: 11.9 KiB, free: 434.4 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/18 07:25:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:50397 (size: 34.9 KiB, free: 434.4 MiB)\n",
      "23/08/18 07:25:04 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4252 ms on localhost (executor 2) (1/1)\n",
      "23/08/18 07:25:04 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "23/08/18 07:25:04 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 4.345 s\n",
      "23/08/18 07:25:04 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/08/18 07:25:04 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished\n",
      "23/08/18 07:25:04 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 4.361716 s\n",
      "23/08/18 07:25:05 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/08/18 07:25:05 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/08/18 07:25:05 INFO FileSourceStrategy: Output Data Schema: struct<>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/18 07:25:05 INFO CodeGenerator: Code generated in 38.654959 ms\n",
      "23/08/18 07:25:05 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.0 KiB, free 433.7 MiB)\n",
      "23/08/18 07:25:05 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 433.7 MiB)\n",
      "23/08/18 07:25:05 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:50387 (size: 34.9 KiB, free: 434.3 MiB)\n",
      "23/08/18 07:25:05 INFO SparkContext: Created broadcast 4 from count at NativeMethodAccessorImpl.java:0\n",
      "23/08/18 07:25:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/08/18 07:25:05 INFO DAGScheduler: Registering RDD 13 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "23/08/18 07:25:05 INFO DAGScheduler: Got map stage job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/08/18 07:25:05 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0)\n",
      "23/08/18 07:25:05 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/08/18 07:25:05 INFO DAGScheduler: Missing parents: List()\n",
      "23/08/18 07:25:05 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/08/18 07:25:05 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 16.0 KiB, free 433.6 MiB)\n",
      "23/08/18 07:25:05 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 433.6 MiB)\n",
      "23/08/18 07:25:05 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:50387 (size: 8.2 KiB, free: 434.3 MiB)\n",
      "23/08/18 07:25:05 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\n",
      "23/08/18 07:25:05 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/08/18 07:25:05 INFO YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "23/08/18 07:25:05 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (localhost, executor 1, partition 0, PROCESS_LOCAL, 4914 bytes) taskResourceAssignments Map()\n",
      "23/08/18 07:25:05 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:50394 (size: 8.2 KiB, free: 434.4 MiB)\n",
      "23/08/18 07:25:05 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:50394 (size: 34.9 KiB, free: 434.3 MiB)\n",
      "23/08/18 07:25:05 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 536 ms on localhost (executor 1) (1/1)\n",
      "23/08/18 07:25:05 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "23/08/18 07:25:05 INFO DAGScheduler: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 0.588 s\n",
      "23/08/18 07:25:05 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/08/18 07:25:05 INFO DAGScheduler: running: Set()\n",
      "23/08/18 07:25:05 INFO DAGScheduler: waiting: Set()\n",
      "23/08/18 07:25:05 INFO DAGScheduler: failed: Set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/18 07:25:05 INFO CodeGenerator: Code generated in 35.117833 ms\n",
      "23/08/18 07:25:05 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "23/08/18 07:25:05 INFO DAGScheduler: Got job 3 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/08/18 07:25:05 INFO DAGScheduler: Final stage: ResultStage 4 (count at NativeMethodAccessorImpl.java:0)\n",
      "23/08/18 07:25:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
      "23/08/18 07:25:05 INFO DAGScheduler: Missing parents: List()\n",
      "23/08/18 07:25:06 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/08/18 07:25:06 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.1 KiB, free 433.6 MiB)\n",
      "23/08/18 07:25:06 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.6 MiB)\n",
      "23/08/18 07:25:06 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 127.0.0.1:50387 (size: 5.5 KiB, free: 434.3 MiB)\n",
      "23/08/18 07:25:06 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\n",
      "23/08/18 07:25:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/08/18 07:25:06 INFO YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "23/08/18 07:25:06 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (localhost, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\n",
      "23/08/18 07:25:06 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:50394 (size: 5.5 KiB, free: 434.3 MiB)\n",
      "23/08/18 07:25:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 127.0.0.1:50392\n",
      "23/08/18 07:25:06 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 784 ms on localhost (executor 1) (1/1)\n",
      "23/08/18 07:25:06 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "23/08/18 07:25:06 INFO DAGScheduler: ResultStage 4 (count at NativeMethodAccessorImpl.java:0) finished in 0.806 s\n",
      "23/08/18 07:25:06 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/08/18 07:25:06 INFO YarnScheduler: Killing all running tasks in stage 4: Stage finished\n",
      "23/08/18 07:25:06 INFO DAGScheduler: Job 3 finished: count at NativeMethodAccessorImpl.java:0, took 0.822264 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "256"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = spark.read.csv(\"hdfs://localhost:9000/data/2015-summary.csv\", header=True, inferSchema=True)\n",
    "data.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T22:25:06.831115Z",
     "start_time": "2023-08-17T22:24:52.774176Z"
    }
   },
   "id": "b9c7e41e32998a2e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: int]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.repartition(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T22:25:15.109235Z",
     "start_time": "2023-08-17T22:25:15.094627Z"
    }
   },
   "id": "dab85489059daeb1"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/18 07:25:22 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/08/18 07:25:22 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/08/18 07:25:22 INFO FileSourceStrategy: Output Data Schema: struct<DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: int ... 1 more fields>\n",
      "23/08/18 07:25:22 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 201.0 KiB, free 433.4 MiB)\n",
      "23/08/18 07:25:22 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 433.4 MiB)\n",
      "23/08/18 07:25:22 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:50387 (size: 34.9 KiB, free: 434.2 MiB)\n",
      "23/08/18 07:25:22 INFO SparkContext: Created broadcast 7 from take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/3540522002.py:1\n",
      "23/08/18 07:25:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/08/18 07:25:22 INFO SparkContext: Starting job: take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/3540522002.py:1\n",
      "23/08/18 07:25:22 INFO DAGScheduler: Got job 4 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/3540522002.py:1) with 1 output partitions\n",
      "23/08/18 07:25:22 INFO DAGScheduler: Final stage: ResultStage 5 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/3540522002.py:1)\n",
      "23/08/18 07:25:22 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/08/18 07:25:22 INFO DAGScheduler: Missing parents: List()\n",
      "23/08/18 07:25:22 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[19] at take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/3540522002.py:1), which has no missing parents\n",
      "23/08/18 07:25:22 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 10.5 KiB, free 433.4 MiB)\n",
      "23/08/18 07:25:22 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 433.4 MiB)\n",
      "23/08/18 07:25:22 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 127.0.0.1:50387 (size: 5.8 KiB, free: 434.2 MiB)\n",
      "23/08/18 07:25:22 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\n",
      "23/08/18 07:25:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[19] at take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/3540522002.py:1) (first 15 tasks are for partitions Vector(0))\n",
      "23/08/18 07:25:22 INFO YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "23/08/18 07:25:22 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (localhost, executor 2, partition 0, PROCESS_LOCAL, 4925 bytes) taskResourceAssignments Map()\n",
      "23/08/18 07:25:22 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:50397 (size: 5.8 KiB, free: 434.3 MiB)\n",
      "23/08/18 07:25:22 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:50397 (size: 34.9 KiB, free: 434.3 MiB)\n",
      "23/08/18 07:25:22 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 315 ms on localhost (executor 2) (1/1)\n",
      "23/08/18 07:25:22 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "23/08/18 07:25:22 INFO DAGScheduler: ResultStage 5 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/3540522002.py:1) finished in 0.333 s\n",
      "23/08/18 07:25:22 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/08/18 07:25:22 INFO YarnScheduler: Killing all running tasks in stage 5: Stage finished\n",
      "23/08/18 07:25:22 INFO DAGScheduler: Job 4 finished: take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/3540522002.py:1, took 0.342214 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataframe에서 csv파일을 읽어 로컬 배열이나 리스트 형태로 변환\n",
    "\n",
    "data.take(3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T22:25:22.616632Z",
     "start_time": "2023-08-17T22:25:22.151764Z"
    }
   },
   "id": "e7efe31226ddef65"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/18 07:25:41 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/08/18 07:25:41 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/08/18 07:25:41 INFO FileSourceStrategy: Output Data Schema: struct<DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: int ... 1 more fields>\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#19 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#19 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=63]\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#17,ORIGIN_COUNTRY_NAME#18,count#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://localhost:9000/data/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "data.sort('count').explain()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T22:25:42.015199Z",
     "start_time": "2023-08-17T22:25:41.930415Z"
    }
   },
   "id": "136f1f9cc37f5d21"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView(\"flight_data_2015\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T22:26:28.889398Z",
     "start_time": "2023-08-17T22:26:28.808449Z"
    }
   },
   "id": "ee1a39e43d11b648"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/18 07:27:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:50387 in memory (size: 34.9 KiB, free: 434.2 MiB)\n",
      "23/08/18 07:27:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:50394 in memory (size: 34.9 KiB, free: 434.3 MiB)\n",
      "23/08/18 07:27:16 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 127.0.0.1:50387 in memory (size: 34.9 KiB, free: 434.3 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "DataFrame[dest_country_name: string, count(1): bigint]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select dest_country_name, count(1)\n",
    "from flight_data_2015\n",
    "group by dest_country_name\n",
    "''')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T22:27:16.485128Z",
     "start_time": "2023-08-17T22:27:16.233551Z"
    }
   },
   "id": "c2ad1931fee1ac4d"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/18 07:29:00 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/08/18 07:29:00 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/08/18 07:29:00 INFO FileSourceStrategy: Output Data Schema: struct<DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: int ... 1 more fields>\n",
      "23/08/18 07:29:00 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 201.0 KiB, free 434.2 MiB)\n",
      "23/08/18 07:29:00 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 434.2 MiB)\n",
      "23/08/18 07:29:00 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 127.0.0.1:50387 (size: 34.9 KiB, free: 434.4 MiB)\n",
      "23/08/18 07:29:00 INFO SparkContext: Created broadcast 11 from take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/906759371.py:1\n",
      "23/08/18 07:29:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/08/18 07:29:00 INFO SparkContext: Starting job: take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/906759371.py:1\n",
      "23/08/18 07:29:00 INFO DAGScheduler: Got job 6 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/906759371.py:1) with 1 output partitions\n",
      "23/08/18 07:29:00 INFO DAGScheduler: Final stage: ResultStage 7 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/906759371.py:1)\n",
      "23/08/18 07:29:00 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/08/18 07:29:00 INFO DAGScheduler: Missing parents: List()\n",
      "23/08/18 07:29:00 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[26] at take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/906759371.py:1), which has no missing parents\n",
      "23/08/18 07:29:00 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 10.5 KiB, free 434.2 MiB)\n",
      "23/08/18 07:29:00 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 434.2 MiB)\n",
      "23/08/18 07:29:00 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 127.0.0.1:50387 (size: 5.8 KiB, free: 434.4 MiB)\n",
      "23/08/18 07:29:00 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\n",
      "23/08/18 07:29:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[26] at take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/906759371.py:1) (first 15 tasks are for partitions Vector(0))\n",
      "23/08/18 07:29:00 INFO YarnScheduler: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "23/08/18 07:29:00 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (localhost, executor 1, partition 0, PROCESS_LOCAL, 4925 bytes) taskResourceAssignments Map()\n",
      "23/08/18 07:29:00 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:50394 (size: 5.8 KiB, free: 434.4 MiB)\n",
      "23/08/18 07:29:00 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:50394 (size: 34.9 KiB, free: 434.4 MiB)\n",
      "23/08/18 07:29:00 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 194 ms on localhost (executor 1) (1/1)\n",
      "23/08/18 07:29:00 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "23/08/18 07:29:00 INFO DAGScheduler: ResultStage 7 (take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/906759371.py:1) finished in 0.209 s\n",
      "23/08/18 07:29:00 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/08/18 07:29:00 INFO YarnScheduler: Killing all running tasks in stage 7: Stage finished\n",
      "23/08/18 07:29:00 INFO DAGScheduler: Job 6 finished: take at /var/folders/pv/tlww36x50mzbwkwxz5g2c0x40000gn/T/ipykernel_5554/906759371.py:1, took 0.220354 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T22:29:00.746642Z",
     "start_time": "2023-08-17T22:29:00.400081Z"
    }
   },
   "id": "78dd9ccf04196856"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/18 07:35:31 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040\n",
      "23/08/18 07:35:31 INFO YarnClientSchedulerBackend: Interrupting monitor thread\n",
      "23/08/18 07:35:31 INFO YarnClientSchedulerBackend: Shutting down all executors\n",
      "23/08/18 07:35:31 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\n",
      "23/08/18 07:35:31 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped\n",
      "23/08/18 07:35:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "23/08/18 07:35:31 INFO MemoryStore: MemoryStore cleared\n",
      "23/08/18 07:35:31 INFO BlockManager: BlockManager stopped\n",
      "23/08/18 07:35:31 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "23/08/18 07:35:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "23/08/18 07:35:31 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-17T22:35:31.585570Z",
     "start_time": "2023-08-17T22:35:31.071691Z"
    }
   },
   "id": "81cac0b9445cce4c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "faef36acba51ae96"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
