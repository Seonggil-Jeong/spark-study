{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-16T20:21:51.435643Z",
     "start_time": "2023-08-16T20:21:50.787598Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/17 05:27:27 WARN Utils: Your hostname, SeongGils-MacBook-Pro-2.local resolves to a loopback address: 127.0.0.1; using 192.168.219.132 instead (on interface en0)\n",
      "23/08/17 05:27:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/17 05:27:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/08/17 05:27:30 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"MoviesAnalysis\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T20:27:56.576075Z",
     "start_time": "2023-08-16T20:27:24.654873Z"
    }
   },
   "id": "5291bca0e5069330"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4c766dc962897272"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "movie_data = spark \\\n",
    "    .read \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"hdfs://localhost:9000/data/movies/KC_KOBIS_BOX_OFFIC_MOVIE_INFO_202105.csv\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T20:31:02.360684Z",
     "start_time": "2023-08-16T20:30:59.319330Z"
    }
   },
   "id": "17529857d8e49c39"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Row(SN=1, MOVIE_NM='분노의 질주: 더 얼티메이트', MNG_NM='저스틴 린', MAKR_NM=None, IMPORT_CMPNY_NM='유니버설픽쳐스인터내셔널 코리아(유)', DISTB_CMPNY_NM='유니버설픽쳐스인터내셔널 코리아(유)', OPEN_DE=20210519, MOVIE_TY_NM='개봉영화', MOVIE_STLE_NM='장편', NLTY_NM='미국', WNTY_SCREEN_CO=2296, WNTY_SELNG_AM=17268076580, WNTY_AUDE_CO=1790155, SU_SELNG_AM=4197011990, SU_AUDE_CO=423112, GENRE_NM='액션', GRAD_NM='12세이상관람가', MOVIE_SE='일반영화')"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T20:31:38.428041Z",
     "start_time": "2023-08-16T20:31:37.630796Z"
    }
   },
   "id": "15a58eeb139308e1"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "### 함수를 통한 실행계획과 SQL의 실행 계획 차이 비교\n",
    "\n",
    "# csv -> View\n",
    "movie_data.createOrReplaceTempView(\"movie_data\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T20:33:42.167597Z",
     "start_time": "2023-08-16T20:33:42.141082Z"
    }
   },
   "id": "214a208c52f6ed78"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[NLTY_NM#79], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(NLTY_NM#79, 200), ENSURE_REQUIREMENTS, [plan_id=125]\n",
      "      +- HashAggregate(keys=[NLTY_NM#79], functions=[partial_count(1)])\n",
      "         +- FileScan csv [NLTY_NM#79] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://localhost:9000/data/movies/KC_KOBIS_BOX_OFFIC_MOVIE_INFO_202105..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<NLTY_NM:string>\n"
     ]
    }
   ],
   "source": [
    "# 함수를 통한 호출 실행계획\n",
    "movie_data.groupBy(\"NLTY_NM\") \\\n",
    "    .count() \\\n",
    "    .explain()\n",
    "\n",
    "## daptiveSparkPlan isFinalPlan=false:\n",
    "# 이 부분은 적응형 실행 계획의 시작\n",
    "# 적응형 실행 계획은 실행 중에 최적의 처리 경로를 선택할 수 있는 기능을 제공합니다. isFinalPlan=false는 이 계획이 아직 최종적인 것이 아니라는 것\n",
    "\n",
    "## HashAggregate(keys=[NLTY_NM#79], functions=[count(1)]):\n",
    "# 데이터를 그룹화, 집계 함수를 적용 group by NLTY_NM, count() 중간 결과로 활용\n",
    "\n",
    "## Exchange hashpartitioning(NLTY_NM#79, 200), ENSURE_REQUIREMENTS, [plan_id=86]:\n",
    "# 데이터의 분산처리를 위해 데이터를 교환하는 단계,  'NLTY_NM' 열을 기준으로 해시 파티셔닝을 수행하며, 200개의 파티션으로 데이터를 분산\n",
    "# ENSURE_REQUIREMENTS는 실행 환경의 요구 사항을 충족시키기 위한 추가 작업을 수행\n",
    "\n",
    "## ashAggregate(keys=[NLTY_NM#79], functions=[partial_count(1)]):\n",
    "# 이 단계는 이전 단계에서 분산된 데이터를 다시 그룹화하고 집계 함수를 적용\n",
    "# partial_count(1) 집계 함수를 사용하여 각 그룹 내의 레코드 수를 세는 작업을 수행합니다. 'NLTY_NM' 열을 기준으로 그룹화\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T20:39:42.057164Z",
     "start_time": "2023-08-16T20:39:41.972787Z"
    }
   },
   "id": "cdc5268d5783cac5"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[NLTY_NM#79], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(NLTY_NM#79, 200), ENSURE_REQUIREMENTS, [plan_id=138]\n",
      "      +- HashAggregate(keys=[NLTY_NM#79], functions=[partial_count(1)])\n",
      "         +- FileScan csv [NLTY_NM#79] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://localhost:9000/data/movies/KC_KOBIS_BOX_OFFIC_MOVIE_INFO_202105..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<NLTY_NM:string>\n"
     ]
    }
   ],
   "source": [
    "# Query를 통한 호출 실행계획\n",
    "spark.sql(\"\"\"\n",
    "SELECT NLTY_NM, count(1)\n",
    "from movie_data as t1 group by t1.NLTY_NM \n",
    "\"\"\").explain()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T20:41:25.891545Z",
     "start_time": "2023-08-16T20:41:25.815973Z"
    }
   },
   "id": "785da1a9c3d9c3b7"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|NLTY_NM|screen_total|\n",
      "+-------+------------+\n",
      "|   미국|       14242|\n",
      "|   한국|       12075|\n",
      "|   일본|        3169|\n",
      "|   영국|        1003|\n",
      "| 헝가리|         691|\n",
      "+-------+------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "select t1.NLTY_NM,\n",
    "sum(t1.WNTY_SCREEN_CO) as screen_total\n",
    "from movie_data as t1\n",
    "group by t1.NLTY_NM\n",
    "order by screen_total desc\n",
    "limit 5\n",
    "\"\"\"\n",
    "spark.sql(q).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T20:44:39.144955Z",
     "start_time": "2023-08-16T20:44:37.967766Z"
    }
   },
   "id": "2b779bc9bebfea8d"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- TakeOrderedAndProject(limit=5, orderBy=[screen_total#422L DESC NULLS LAST], output=[NLTY_NM#79,screen_total#422L])\n",
      "   +- HashAggregate(keys=[NLTY_NM#79], functions=[sum(WNTY_SCREEN_CO#80)])\n",
      "      +- Exchange hashpartitioning(NLTY_NM#79, 200), ENSURE_REQUIREMENTS, [plan_id=445]\n",
      "         +- HashAggregate(keys=[NLTY_NM#79], functions=[partial_sum(WNTY_SCREEN_CO#80)])\n",
      "            +- FileScan csv [NLTY_NM#79,WNTY_SCREEN_CO#80] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://localhost:9000/data/movies/KC_KOBIS_BOX_OFFIC_MOVIE_INFO_202105..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<NLTY_NM:string,WNTY_SCREEN_CO:int>\n"
     ]
    }
   ],
   "source": [
    "spark.sql(q).explain()\n",
    "\n",
    "### dataframe으로 sql 실행 시 실행 계획\n",
    "# csv -(read)> dataframe -(group by)> dataframe -(sum)> dataframe -(change column name)> dataframe -(sort)> dataframe -(limit)> dataframe -(collect)> Array()  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T20:47:42.731310Z",
     "start_time": "2023-08-16T20:47:42.663946Z"
    }
   },
   "id": "cc0f73a390a453e5"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "list"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark.sql(q).collect())\n",
    "\n",
    "## before show() : 이전까지 트렌스포메이션은 하나 또는 그룹의 데이터 프레임을 반환\n",
    "## after show() : 이후에는 이제까지 나온 Dataframe을 사용하는 언어ㅔ 맞게 array or list로 변환"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T20:49:45.915834Z",
     "start_time": "2023-08-16T20:49:45.373372Z"
    }
   },
   "id": "601cdf7138eb8295"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "### 타입형/비타입형 API의 개념과 차이점\n",
    "### 스파크가 구조적 API의 데이터 흐름을 해석, 클러스터 실행 방식\n",
    "\n",
    "## 구조적 API에는 비타입형(Dataframe)과 타입형(DataSet)으로 구분\n",
    "## 이 차이는 스키마(data 구성요소)에 명시된 데이터 타입의 일치여부를 언제 확인하는지에 따라 구분됨\n",
    "\n",
    "# Dataset : 컴파일타임에서 데이터 요소와 데이터 타입의 일치 여부를 확인,\n",
    "# 따라서 명시적으로 데이터타입을 선언하는 JVM 기반 언어에서만 사용 가능\n",
    "\n",
    "# Dataframe : Row 타입으로 구성된 Dataset\n",
    "\n",
    "### Row타입이란?\n",
    "# 스파크가 사용하는 \"연산에 최적화된 인메모리 포맷\"의 내부적인 표현방식\n",
    "# Row타입 사용시 가비지 컬렉션과 객체 초기화 부하가 있는 JVM 데이터 타입을 사용하는 대신 자체 데이터\n",
    "# 포맷을 사용하여\n",
    "# 결론적으로 Dataframe 사용시 매우 효율적인 연산이 가능"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T20:58:58.649241Z",
     "start_time": "2023-08-16T20:58:58.645184Z"
    }
   },
   "id": "7d9510f062a20d5d"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "## 구조적 API의 실행 과정\n",
    "# 1. DataFrame/Dataset/SQL을 통해 코드 작성\n",
    "#2. 정상적인 코드라면 스파크(카탈리스트 옵티마이저)가 논리적 실행계획으로 변환\n",
    "# 3. 스파크는 논리적 실행 계획을 물리적 실행 계획으로 변환: 이때, 추가적인 최적화 방법이 있는지 확인\n",
    "# 4. 스파크는 클러스터에서 물리적 실행 계획(RDD 처리)을 실행합니다."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T21:00:25.953593Z",
     "start_time": "2023-08-16T21:00:25.935869Z"
    }
   },
   "id": "eccc01cfac804f5c"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "## 논리적 실행 계획 : 논리적 실행계획 단계에서는 추상적 트랜스포메이션만 표현(드라이버나 익스큐터의 정보는 고려X)\n",
    "\n",
    "\n",
    "# 검증 전 논리적 실행 계획 : 사용자가 작성한 코드를 유효성과 테이블의 컬럼 존재 여부만 판단하여 논리적 실행 계획으로 변환 (컴파일과 같은 내용)\n",
    "# 검증된 논리적 실행 계획 : 카탈로그, 모든 테이블의 저장소 그리고 DataFrame 정보등 실제 데이터를 활용하여 스파크 분석기(analyzer)가 검증한 논리적 실행 계획\n",
    "# 최적화된 논리적 실행 계획 : 검증된 논리적 실행계획을 Catalyst Optimizer가 조건절 푸쉬 다운이나 선택절 구문을 이용해 논리적 실행 계획을 최적화함"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T21:01:24.544371Z",
     "start_time": "2023-08-16T21:01:24.525040Z"
    }
   },
   "id": "346ceebda6c52818"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "## 물리적 실행 계획 : 논리적 실행 계획을 클러스트 환경에서 실행하는 방법을 정의\n",
    "# 다양한 물리적 계획 중 비용 모델을 이용하여 비교 후 최적의 전략을 클러스터에 전달, 실행"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T21:02:09.415753Z",
     "start_time": "2023-08-16T21:02:09.400195Z"
    }
   },
   "id": "5325bcc64b2f2544"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "99af904973e087"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
